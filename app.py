import streamlit as st
import os
import logging
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import faiss
import pickle
import pandas as pd
import numpy as np
from pathlib import Path

# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ LangChain imports
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()


# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
class Config:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    # –î–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º OpenAI –Ω–∞–ø—Ä—è–º—É—é
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    # –î–ª—è LLM –∏—Å–ø–æ–ª—å–∑—É–µ–º OpenRouter
    OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
    OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"

    EMBEDDING_MODEL = "text-embedding-3-large"
    LLM_MODEL = "deepseek/deepseek-chat"

    # –ü—É—Ç–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
    FAISS_INDEX_DIR = "/Users/Tosha/Desktop/Projects/asksunna/asksunna/faiss_index"
    HADITH_CHUNKS_PATH = "/Users/Tosha/Desktop/Projects/asksunna/asksunna/DB/hadith_chunks.parquet"
    HADITH_EMBEDDINGS_PATH = "/Users/Tosha/Desktop/Projects/asksunna/asksunna/DB/hadith_embeddings.parquet"

    MAX_RETRIEVAL_DOCS = 5
    LLM_TEMPERATURE = 0.1

    @classmethod
    def validate(cls):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if not cls.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
        if not cls.OPENROUTER_API_KEY:
            raise ValueError("OPENROUTER_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
        if not os.path.exists(cls.FAISS_INDEX_DIR):
            raise ValueError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ {cls.FAISS_INDEX_DIR} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        if not os.path.exists(cls.HADITH_CHUNKS_PATH):
            raise ValueError(f"–§–∞–π–ª —Å —á–∞–Ω–∫–∞–º–∏ —Ö–∞–¥–∏—Å–æ–≤ {cls.HADITH_CHUNKS_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω")


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
try:
    Config.validate()
except ValueError as e:
    st.error(f"–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
    st.stop()


class MetadataManager:
    """Class for working with hadith metadata"""

    def __init__(self):
        self.hadith_chunks_df = None
        self.load_metadata()

    def load_metadata(self):
        """Load metadata from parquet file"""
        try:
            self.hadith_chunks_df = pd.read_parquet(Config.HADITH_CHUNKS_PATH)
            logger.info(f"Loaded {len(self.hadith_chunks_df)} metadata records")
        except Exception as e:
            logger.error(f"Error loading metadata: {e}")
            raise

    def get_chunk_metadata(self, uid_chunked: str) -> Dict[str, Any]:
        """
        Get complete metadata for chunk by uid_chunked

        Args:
            uid_chunked: unique chunk identifier

        Returns:
            Dictionary with complete metadata
        """
        try:
            row = self.hadith_chunks_df[self.hadith_chunks_df['uid_chunked'] == uid_chunked]

            if row.empty:
                logger.warning(f"Metadata for uid_chunked {uid_chunked} not found")
                return {}

            return row.iloc[0].to_dict()

        except Exception as e:
            logger.error(f"Error getting metadata for {uid_chunked}: {e}")
            return {}


class HadithRetriever:
    """Class for working with vector search on hadiths"""

    def __init__(self):
        self.embeddings = None
        self.vectorstore = None
        self.retriever = None
        self.metadata_manager = MetadataManager()
        self._init_embeddings()

    def _init_embeddings(self):
        """Initialize embedding model via OpenAI API"""
        try:
            self.embeddings = OpenAIEmbeddings(
                model=Config.EMBEDDING_MODEL,
                openai_api_key=Config.OPENAI_API_KEY,
            )
            logger.info("Embedding model successfully initialized via OpenAI API")
        except Exception as e:
            logger.error(f"Error initializing embedding model: {e}")
            raise
            logger.info("–≠–º–±–µ–¥–¥–∏–Ω–≥ –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —á–µ—Ä–µ–∑ OpenAI API")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ –º–æ–¥–µ–ª–∏: {e}")
            raise

    @st.cache_resource
    def load_faiss_index(_self, collection_name: str) -> Optional[FAISS]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–±–æ—Ä–Ω–∏–∫–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º

        Args:
            collection_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Å–±–æ—Ä–Ω–∏–∫–∞ —Ö–∞–¥–∏—Å–æ–≤

        Returns:
            FAISS vectorstore –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            index_path = os.path.join(Config.FAISS_INDEX_DIR, collection_name)

            if not os.path.exists(index_path):
                logger.error(f"–ò–Ω–¥–µ–∫—Å –¥–ª—è —Å–±–æ—Ä–Ω–∏–∫–∞ '{collection_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω: {index_path}")
                return None

            # –ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞
            vectorstore = FAISS.load_local(
                index_path,
                _self.embeddings,
                allow_dangerous_deserialization=True
            )

            logger.info(f"–ò–Ω–¥–µ–∫—Å –¥–ª—è —Å–±–æ—Ä–Ω–∏–∫–∞ '{collection_name}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
            return vectorstore

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è —Å–±–æ—Ä–Ω–∏–∫–∞ '{collection_name}': {e}")
            return None

    def setup_retriever(self, collection_name: str) -> bool:
        """
        –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–±–æ—Ä–Ω–∏–∫–∞

        Args:
            collection_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Å–±–æ—Ä–Ω–∏–∫–∞ —Ö–∞–¥–∏—Å–æ–≤

        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            self.vectorstore = self.load_faiss_index(collection_name)

            if self.vectorstore is None:
                return False

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
            self.retriever = self.vectorstore.as_retriever(
                search_kwargs={"k": Config.MAX_RETRIEVAL_DOCS}
            )

            logger.info(f"–†–µ—Ç—Ä–∏–≤–µ—Ä –¥–ª—è —Å–±–æ—Ä–Ω–∏–∫–∞ '{collection_name}' –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
            return True

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞: {e}")
            return False

    def search_similar_chunks(self, query: str) -> List[Document]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —Å –æ–±–æ–≥–∞—â–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

        Args:
            query: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if not self.retriever:
            logger.error("–†–µ—Ç—Ä–∏–≤–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return []

        try:
            docs = self.retriever.get_relevant_documents(query)

            # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            enriched_docs = []
            for doc in docs:
                uid_chunked = doc.metadata.get('uid_chunked')
                if uid_chunked:
                    full_metadata = self.metadata_manager.get_chunk_metadata(uid_chunked)
                    if full_metadata:
                        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —Å –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                        enriched_doc = Document(
                            page_content=doc.page_content,
                            metadata=full_metadata
                        )
                        enriched_docs.append(enriched_doc)
                    else:
                        enriched_docs.append(doc)
                else:
                    enriched_docs.append(doc)

            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(enriched_docs)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏")
            return enriched_docs

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []


class HadithQAChain:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤"""

    def __init__(self):
        self.llm = None
        self.qa_chain = None
        self._init_llm()

    def _init_llm(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ OpenRouter"""
        try:
            self.llm = ChatOpenAI(
                model=Config.LLM_MODEL,
                openai_api_key=Config.OPENROUTER_API_KEY,
                openai_api_base=Config.OPENROUTER_BASE_URL,
                temperature=Config.LLM_TEMPERATURE,
                max_tokens=1500
            )
            logger.info("LLM –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —á–µ—Ä–µ–∑ OpenRouter")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LLM: {e}")
            raise

    def setup_qa_chain(self, retriever):
        """
        –ù–∞—Å—Ç—Ä–æ–π–∫–∞ QA —Ü–µ–ø–æ—á–∫–∏ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º

        Args:
            retriever: –†–µ—Ç—Ä–∏–≤–µ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        try:
            # Custom prompt for working with hadiths
            prompt_template = """
You are an expert on Islamic hadiths. Use the provided hadith fragments to give an accurate and well-founded answer to the user's question.

Context (hadith fragments):
{context}

Question: {question}

Instructions:
1. Answer accurately and exclusively based on the provided hadiths
2. If there is insufficient information for a complete answer, honestly state this
3. When possible, indicate collection names and hadith numbers
4. Be respectful, accurate, and academic in your approach
5. Answer in the language the question was asked in
6. Structure your answer logically and clearly
7. When quoting, indicate the source in parentheses

Answer:"""

            PROMPT = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )

            # –°–æ–∑–¥–∞–Ω–∏–µ QA —Ü–µ–ø–æ—á–∫–∏
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": PROMPT},
                return_source_documents=True
            )

            logger.info("QA —Ü–µ–ø–æ—á–∫–∞ —É—Å–ø–µ—à–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ QA —Ü–µ–ø–æ—á–∫–∏: {e}")
            raise

    def get_answer(self, question: str) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å

        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
        """
        if not self.qa_chain:
            logger.error("QA —Ü–µ–ø–æ—á–∫–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return {"answer": "–û—à–∏–±–∫–∞: QA —Ü–µ–ø–æ—á–∫–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞", "sources": []}

        try:
            result = self.qa_chain.invoke({"query": question})

            return {
                "answer": result["result"],
                "sources": result["source_documents"]
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return {"answer": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}", "sources": []}


def get_available_collections() -> List[str]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–±–æ—Ä–Ω–∏–∫–æ–≤ —Ö–∞–¥–∏—Å–æ–≤

    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å–±–æ—Ä–Ω–∏–∫–æ–≤
    """
    try:
        index_dir = Path(Config.FAISS_INDEX_DIR)
        if not index_dir.exists():
            return []

        collections = [d.name for d in index_dir.iterdir() if d.is_dir()]
        return sorted(collections)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Å–±–æ—Ä–Ω–∏–∫–æ–≤: {e}")
        return []


def format_source_document(doc: Document, index: int) -> str:
    """
    Format source document for display with complete metadata in English

    Args:
        doc: Document from search
        index: Document number

    Returns:
        Formatted string
    """
    metadata = doc.metadata
    content = doc.page_content  # Show full content, not truncated

    # Safe value retrieval with None and NaN handling
    def safe_get(key, default="Not specified"):
        value = metadata.get(key, default)
        if pd.isna(value) or value is None or value == "":
            return default
        return str(value)

    # Determine if content is Arabic or English
    arabic_text = ""
    english_text = ""

    # Check if the main content is Arabic (contains Arabic characters)
    if any('\u0600' <= char <= '\u06FF' for char in content[:100]):
        arabic_text = content
        english_text = safe_get('text_en', 'English translation not available')
    else:
        english_text = content
        arabic_text = safe_get('text_ar', safe_get('text_ar_chunk', 'Arabic text not available'))

    return f"""
**üìñ Source {index + 1}**

**üîç Basic Information:**
- **Collection:** {safe_get('collection_book')}
- **Author:** {safe_get('author_en')} ({safe_get('author_ar')})
- **Book:** {safe_get('book_title_en')} ({safe_get('book_title_ar')})
- **Chapter:** {safe_get('chapter_title_en')} ({safe_get('chapter_title_ar')})

**üìã Identifiers:**
- **Hadith ID:** {safe_get('hadith_id')}
- **Number in Book:** {safe_get('hadith_id_in_book')}
- **Chunk ID:** {safe_get('uid_chunked')}
- **Chunk Number:** {safe_get('chunk_index')} of {safe_get('chunk_total')}

**üë§ Narrator:** {safe_get('narrator_en')}

**üìÑ English Text:**
{english_text}

**üåê Arabic Text:**
{arabic_text}

---
"""


def display_search_statistics(sources: List[Document]):
    """
    Display statistics for found sources

    Args:
        sources: List of found documents
    """
    if not sources:
        return

    # Collect statistics
    collections = {}
    authors = {}
    books = {}

    for doc in sources:
        metadata = doc.metadata

        collection = metadata.get('collection_book', 'Unknown')
        author = metadata.get('author_en', 'Unknown')
        book = metadata.get('book_title_en', 'Unknown')

        collections[collection] = collections.get(collection, 0) + 1
        authors[author] = authors.get(author, 0) + 1
        books[book] = books.get(book, 0) + 1

    col1, col2, col3 = st.columns(3)

    with col1:
        st.subheader("üìö By Collections")
        for collection, count in collections.items():
            st.text(f"‚Ä¢ {collection}: {count}")

    with col2:
        st.subheader("‚úçÔ∏è By Authors")
        for author, count in authors.items():
            st.text(f"‚Ä¢ {author}: {count}")

    with col3:
        st.subheader("üìñ By Books")
        for book, count in list(books.items())[:5]:  # Show top 5
            book_short = book[:30] + "..." if len(book) > 30 else book
            st.text(f"‚Ä¢ {book_short}: {count}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    st.set_page_config(
        page_title="Hadith QA Retriever",
        page_icon="üìö",
        layout="wide"
    )

    st.title("üìö Hadith QA Retriever")
    st.markdown("*Search for answers in the hadith corpus using AI with complete metadata*")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ —Å–µ—Å—Å–∏–∏
    if 'hadith_retriever' not in st.session_state:
        try:
            with st.spinner("üîÑ Initializing system..."):
                st.session_state.hadith_retriever = HadithRetriever()
                st.session_state.qa_chain = HadithQAChain()
                st.success("‚úÖ System successfully initialized!")
        except Exception as e:
            st.error(f"‚ùå Initialization error: {e}")
            st.stop()

    # Sidebar for settings
    with st.sidebar:
        st.header("‚öôÔ∏è Settings")

        # Collection selection
        collections = get_available_collections()
        if not collections:
            st.error("‚ùå No hadith collections found!")
            st.stop()

        selected_collection = st.selectbox(
            "Select hadith collection:",
            collections,
            help="Choose a collection to search in"
        )

        # Search settings
        st.subheader("üîç Search Parameters")
        max_docs = st.slider("Maximum documents", 1, 10, Config.MAX_RETRIEVAL_DOCS)
        Config.MAX_RETRIEVAL_DOCS = max_docs

        st.markdown("---")
        st.subheader("üìä System Information")
        st.info(f"**Total collections:** {len(collections)}")
        st.info(f"**Embeddings:** {Config.EMBEDDING_MODEL}")
        st.info(f"**LLM:** {Config.LLM_MODEL}")

        st.markdown("---")
        st.subheader("üìö Available Collections")
        for i, collection in enumerate(collections, 1):
            icon = "üéØ" if collection == selected_collection else "üìò"
            st.markdown(f"{icon} {collection}")

    # Main interface
    col1, col2 = st.columns([3, 1])

    with col1:
        st.header("üîç Ask Questions About Hadiths")

        # Example questions
        example_questions = [
            "What do hadiths say about prayer?",
            "Which hadiths mention righteousness?",
            "What is said about fasting in hadiths?",
            "Hadiths about relationships with parents",
            "What is said about knowledge and learning?"
        ]

        selected_example = st.selectbox(
            "Or choose an example:",
            [""] + example_questions,
            help="Select a ready-made question or enter your own"
        )

        # Question input field
        question = st.text_area(
            "Enter your question:",
            value=selected_example if selected_example else "",
            height=100,
            placeholder="For example: What do hadiths say about prayer?"
        )

        # Search button
        search_button = st.button("üîé Find Answer", type="primary", use_container_width=True)

    with col2:
        st.header("üìä Status")
        st.success(f"**Selected:** {selected_collection}")
        st.info(f"**Max results:** {max_docs}")

        if st.button("üîÑ Reload Indexes", use_container_width=True):
            st.cache_resource.clear()
            st.success("Cache cleared!")

    # Process query
    if search_button and question.strip():
        with st.spinner("üîÑ Searching for answer in hadith corpus..."):
            try:
                # Setup retriever for selected collection
                if not st.session_state.hadith_retriever.setup_retriever(selected_collection):
                    st.error(f"‚ùå Error loading index for collection '{selected_collection}'")
                    st.stop()

                # Setup QA chain
                st.session_state.qa_chain.setup_qa_chain(st.session_state.hadith_retriever.retriever)

                # Get answer
                result = st.session_state.qa_chain.get_answer(question)

                # Display results
                st.markdown("---")
                st.header("üí¨ Answer")

                # Answer in a styled container with better visibility
                with st.container():
                    st.markdown(
                        f"""
                        <div style='background-color: #ffffff; padding: 20px; border-radius: 10px; border-left: 5px solid #1f77b4; color: #000000; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>
                        <div style='color: #000000; line-height: 1.6;'>
                        {result["answer"]}
                        </div>
                        </div>
                        """,
                        unsafe_allow_html=True
                    )

                # Statistics and sources
                if result["sources"]:
                    st.markdown("---")
                    st.header("üìä Statistics of Found Sources")
                    display_search_statistics(result["sources"])

                    st.markdown("---")
                    st.header("üìñ Detailed Sources")
                    st.markdown(f"*Found {len(result['sources'])} relevant hadith fragments*")

                    for i, doc in enumerate(result["sources"]):
                        with st.expander(
                                f"Source {i + 1}: {doc.metadata.get('collection_book', 'Unknown')} - "
                                f"Hadith #{doc.metadata.get('hadith_id_in_book', 'N/A')}",
                                expanded=i == 0  # First source open by default
                        ):
                            st.markdown(format_source_document(doc, i))
                else:
                    st.warning("‚ö†Ô∏è No relevant sources found")

            except Exception as e:
                st.error(f"‚ùå An error occurred: {e}")
                logger.error(f"Error in main process: {e}")

    elif search_button and not question.strip():
        st.warning("‚ö†Ô∏è Please enter a question")

    # –§—É—Ç–µ—Ä
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666; padding: 20px;'>
        <h4>üìö Hadith QA Retriever</h4>
        <p><strong>Powered by:</strong> OpenAI Embeddings ‚Ä¢ OpenRouter LLM ‚Ä¢ FAISS Vector Search</p>
        <p><em>Complete hadith metadata database with semantic search</em></p>
        </div>
        """,
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()